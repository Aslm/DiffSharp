<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <!-- 
      The API Overview
 parameters will be replaced with the 
      document title extracted from the <h1> element or
      file name, if there is no <h1> heading
    -->
    <title>API Overview
</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <meta name="author" content="Atılım Güneş Baydin; Barak A. Pearlmutter">
    <meta name="description" content="DiffSharp is an automatic differentiation (AD) library implemented in the F# language by Atılım Güneş Baydin and Barak A. Pearlmutter, mainly for research applications in machine learning, as part of their work at the Brain and Computation Lab, Hamilton Institute, National University of Ireland Maynooth.">

    <script src="https://code.jquery.com/jquery-1.8.0.js"></script>
    <script src="https://code.jquery.com/ui/1.8.23/jquery-ui.js"></script>
    <script src="https://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.1/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link href="https://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.1/css/bootstrap-combined.min.css" rel="stylesheet">
    
    <link type="text/css" rel="stylesheet" href="misc/style.css" />
    <script src="misc/tips.js" type="text/javascript"></script>
    <!-- HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-48900508-3', 'auto');
      ga('require', 'displayfeatures');
      ga('send', 'pageview');
    </script>
  </head>
  <body>
    <div class="container">
      <div class="masthead">
        <ul class="nav nav-pills pull-right">
          <li><a href="http://fsharp.org">fsharp.org</a></li>
        </ul>
        <h3 class="muted">DiffSharp</h3>
      </div>
      <hr />
      <div class="row">
        <div class="span9" id="main">
          <h1>API Overview</h1>

<p>The following table gives an overview of the differentiation API provided by the DiffSharp library.</p>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg th{font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;}
.tg .tg-a60z{font-size:9px;background-color:#ecf4ff}
.tg .tg-1r2d{font-size:9px;background-color:#ecf4ff;text-align:center}
.tg .tg-glis{font-size:9px}
.tg .tg-wcxf{font-size:9px;background-color:#ffffc7;text-align:center}
.tg .tg-aycn{font-size:9px;background-color:#e4ffb3;text-align:center}
.tg .tg-wklz{font-size:9px;background-color:#ecf4ff;color:#000000;text-align:center}
.tg .tg-7dqz{font-weight:bold;font-size:9px}
</style>
<table class="tg">
  <tr>
    <th class="tg-glis"></th>
    <th class="tg-wcxf">diff</th>
    <th class="tg-wcxf">diff2</th>
    <th class="tg-wcxf">diffn</th>
    <th class="tg-aycn">grad</th>
    <th class="tg-aycn">gradv</th>
    <th class="tg-aycn">hessian</th>
    <th class="tg-aycn">hessianv</th>
    <th class="tg-aycn">gradhessian</th>
    <th class="tg-aycn">gradhessianv</th>
    <th class="tg-aycn">laplacian</th>
    <th class="tg-wklz">jacobian</th>
    <th class="tg-1r2d">jacobianv</th>
    <th class="tg-1r2d">jacobianT</th>
    <th class="tg-1r2d">jacobianTv</th>
    <th class="tg-1r2d">jacobianvTv</th>
    <th class="tg-a60z">curl</th>
    <th class="tg-a60z">div</th>
    <th class="tg-a60z">curldiv</th>
  </tr>
  <tr>
    <td class="tg-7dqz">ADF</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf"></td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">ADF2</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">ADFG</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf"></td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">ADFGH</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf"></td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">ADFN</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">ADFR</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">ADR</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf"></td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">XA</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">A</td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
  <tr>
    <td class="tg-7dqz">N</td>
    <td class="tg-wcxf">A</td>
    <td class="tg-wcxf">A</td>
    <td class="tg-wcxf"></td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-aycn">A</td>
    <td class="tg-wklz">A</td>
    <td class="tg-1r2d">A</td>
    <td class="tg-1r2d">A</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">A</td>
    <td class="tg-1r2d">A</td>
    <td class="tg-1r2d">A</td>
  </tr>
  <tr>
    <td class="tg-7dqz">S</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-wcxf">X</td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-aycn"></td>
    <td class="tg-aycn">X</td>
    <td class="tg-wklz">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d"></td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
    <td class="tg-1r2d">X</td>
  </tr>
</table>

<p><strong>Yellow</strong>: For scalar-to-scalar functions; <strong>Green</strong>: For vector-to-scalar functions; <strong>Blue</strong>: For vector-to-vector functions</p>

<p><strong>X</strong>: Exact value; <strong>A</strong>: Numerical approximation; <strong>XA</strong>: Exact gradient, approximated Hessian</p>

<p><strong>ADF</strong>: DiffSharp.AD.Forward; <strong>ADF2</strong>: DiffSharp.AD.Forward2; <strong>ADFG</strong>: DiffSharp.AD.ForwardG; <strong>ADFGH</strong>: DiffSharp.AD.ForwardGH; <strong>ADFN</strong>: DiffSharp.AD.ForwardN; <strong>ADFR</strong>: DiffSharp.AD.ForwardReverse; <strong>ADR</strong>: DiffSharp.AD.Reverse; <strong>N</strong>: DiffSharp.Numerical; <strong>S</strong>: DiffSharp.Symbolic</p>

<h2>Differentiation Operations and Variants</h2>

<p>The operations summarized in the above table have <em>prime-suffixed</em> variants (e.g. <strong>diff</strong> and <strong>diff'</strong> ) that return tuples containing the value of the original function together with the value of the desired operation. This is advantageous in the majority of AD operations, since the original function value is almost always already computed as a by-product of AD computations, providing a performance advantage.</p>

<table class="pre"><tr><td class="lines"><pre class="fssnip">
<span class="l">1: </span>
<span class="l">2: </span>
<span class="l">3: </span>
<span class="l">4: </span>
<span class="l">5: </span>
<span class="l">6: </span>
<span class="l">7: </span>
<span class="l">8: </span>
</pre>
</td>
<td class="snippet"><pre class="fssnip">
<span class="c">// Use forward mode AD</span>
<span class="k">open</span> <span onmouseout="hideTip(event, 'fs1', 1)" onmouseover="showTip(event, 'fs1', 1)" class="i">DiffSharp</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs2', 2)" onmouseover="showTip(event, 'fs2', 2)" class="i">AD</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs3', 3)" onmouseover="showTip(event, 'fs3', 3)" class="i">Forward</span>

<span class="c">// Derivative of Sin(Sqrt(x)) at x = 2</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs4', 4)" onmouseover="showTip(event, 'fs4', 4)" class="i">a</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs5', 5)" onmouseover="showTip(event, 'fs5', 5)" class="i">diff</span> (<span class="k">fun</span> <span onmouseout="hideTip(event, 'fs6', 6)" onmouseover="showTip(event, 'fs6', 6)" class="i">x</span> <span class="k">-&gt;</span> <span onmouseout="hideTip(event, 'fs7', 7)" onmouseover="showTip(event, 'fs7', 7)" class="i">sin</span> (<span onmouseout="hideTip(event, 'fs8', 8)" onmouseover="showTip(event, 'fs8', 8)" class="i">sqrt</span> <span onmouseout="hideTip(event, 'fs6', 9)" onmouseover="showTip(event, 'fs6', 9)" class="i">x</span>)) <span class="n">2.</span>

<span class="c">// (Original value, derivative) of Sin(Sqrt(x)) at x = 2</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs9', 10)" onmouseover="showTip(event, 'fs9', 10)" class="i">b</span>, <span onmouseout="hideTip(event, 'fs10', 11)" onmouseover="showTip(event, 'fs10', 11)" class="i">c</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs11', 12)" onmouseover="showTip(event, 'fs11', 12)" class="i">diff&#39;</span> (<span class="k">fun</span> <span onmouseout="hideTip(event, 'fs6', 13)" onmouseover="showTip(event, 'fs6', 13)" class="i">x</span> <span class="k">-&gt;</span> <span onmouseout="hideTip(event, 'fs7', 14)" onmouseover="showTip(event, 'fs7', 14)" class="i">sin</span> (<span onmouseout="hideTip(event, 'fs8', 15)" onmouseover="showTip(event, 'fs8', 15)" class="i">sqrt</span> <span onmouseout="hideTip(event, 'fs6', 16)" onmouseover="showTip(event, 'fs6', 16)" class="i">x</span>)) <span class="n">2.</span></pre>
</td>
</tr>
</table>

<p>Currently, the library provides the following operations:</p>

<h5>diff : <span class="math">\(\color{red}{(\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>diff f x</code></strong> returns the first derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, and <span class="math">\(x \in \mathbb{R}\)</span>, this gives the derivative evaluated at <span class="math">\(x\)</span></p>

<p><span class="math">\[  \left. \frac{d}{da} f(a) \right|_{a\; =\; x} \; .\]</span></p>

<hr />

<h5>diff' : <span class="math">\(\color{red}{(\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{(\mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>diff' f x</code></strong> returns the original value and the first derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>diff2 : <span class="math">\(\color{red}{(\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>diff2 f x</code></strong> returns the second derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, and <span class="math">\(x \in \mathbb{R}\)</span>, this gives the second derivative evaluated at <span class="math">\(x\)</span></p>

<p><span class="math">\[  \left. \frac{d^2}{da^2} f(a) \right|_{a\; =\; x} \; .\]</span></p>

<hr />

<h5>diff2' : <span class="math">\(\color{red}{(\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{(\mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>diff2' f x</code></strong> returns the original value and the second derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>diff2'' : <span class="math">\(\color{red}{(\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{(\mathbb{R} \times \mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>diff2'' f x</code></strong> returns the original value, the first derivative, and the second derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>diffn : <span class="math">\(\color{red}{\mathbb{R} \to (\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>diffn n f x</code></strong> returns the <code>n</code>-th derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For <span class="math">\(n \in \mathbb{N}\)</span>, a function <span class="math">\(f(a): \mathbb{R} \to \mathbb{R}\)</span>, and <span class="math">\(x \in \mathbb{R}\)</span>, this gives the n-th derivative evaluated at <span class="math">\(x\)</span></p>

<p><span class="math">\[  \left. \frac{d^n}{da^n} f(a) \right|_{a\; =\; x} \; .\]</span></p>

<hr />

<h5>diffn' : <span class="math">\(\color{red}{\mathbb{R} \to (\mathbb{R} \to \mathbb{R}) \to \mathbb{R}} \to \color{blue}{(\mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>diffn' n f x</code></strong> returns the original value and the <code>n</code>-th derivative of a scalar-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>grad : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^n}\)</span></h5>

<p><strong><code>grad f x</code></strong> returns the <a href="http://en.wikipedia.org/wiki/Gradient">gradient</a> of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this gives the gradient evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \nabla f \right)_\mathbf{x} = \left. \left[ \frac{\partial f}{{\partial a}_1}, \dots, \frac{\partial f}{{\partial a}_n} \right] \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<hr />

<h5>grad' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>grad' f x</code></strong> returns the original value and the gradient of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>gradv : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>gradv f x v</code></strong> returns the <a href="http://en.wikipedia.org/wiki/Directional_derivative">gradient-vector product</a> (directional derivative) of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<p>For a function <span class="math">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x}, \mathbf{v} \in \mathbb{R}^n\)</span>, this gives the dot product of the gradient of <span class="math">\(f\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \nabla f \right)_\mathbf{x} \cdot \mathbf{v} \; .\]</span></p>

<p>This value can be computed efficiently by the <strong>DiffSharp.AD.Forward</strong> module in one forward evaluation of the function, without computing the full gradient.</p>

<hr />

<h5>gradv' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>gradv' f x v</code></strong> returns the original value and the gradient-vector product (directional derivative) of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>hessian : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^{n \times n}}\)</span></h5>

<p><strong><code>hessian f x</code></strong> returns the <a href="http://en.wikipedia.org/wiki/Hessian_matrix">Hessian</a> of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this gives the Hessian matrix evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \mathbf{H}_f \right)_\mathbf{x} = \left. \begin{bmatrix}
                                           \frac{\partial ^2 f}{\partial a_1^2} &amp; \frac{\partial ^2 f}{\partial a_1 \partial a_2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_1 \partial a_n} \\
                                           \frac{\partial ^2 f}{\partial a_2 \partial a_1} &amp; \frac{\partial ^2 f}{\partial a_2^2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_2 \partial a_n} \\
                                           \vdots  &amp; \vdots  &amp; \ddots &amp; \vdots  \\
                                           \frac{\partial ^2 f}{\partial a_n \partial a_1} &amp; \frac{\partial ^2 f}{\partial a_n \partial a_2} &amp; \cdots &amp; \frac{\partial ^2 f}{\partial a_n^2}
                                          \end{bmatrix} \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<hr />

<h5>hessian' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R}^{n \times n})}\)</span></h5>

<p><strong><code>hessian' f x</code></strong> returns the original value and the Hessian of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>hessianv : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^n}\)</span></h5>

<p><strong><code>hessianv f x v</code></strong> returns the <a href="http://en.wikipedia.org/wiki/Hessian_automatic_differentiation">Hessian-vector product</a> of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<p>For a function <span class="math">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x}, \mathbf{v} \in \mathbb{R}^n\)</span>, this gives the multiplication of the Hessian matrix of <span class="math">\(f\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \mathbf{H}_f \right)_\mathbf{x} \; \mathbf{v} \; .\]</span></p>

<p>This value can be computed efficiently by the <strong>DiffSharp.AD.ForwardReverse</strong> module using one forward and one reverse evaluation of the function, in a matrix-free way (without computing the full Hessian matrix).</p>

<hr />

<h5>hessianv' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>hessianv' f x v</code></strong> returns the original value and the Hessian-vector product of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>gradhessian : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^n \times \mathbb{R}^{n \times n})}\)</span></h5>

<p><strong><code>gradhessian f x</code></strong> returns the gradient and the Hessian of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>gradhessian' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R}^n \times \mathbb{R}^{n \times n})}\)</span></h5>

<p><strong><code>gradhessian' f x</code></strong> returns the original value, the gradient, and the Hessian of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>gradhessianv : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>gradhessianv f x v</code></strong> returns the gradient-vector product (directional derivative) and the Hessian-vector product of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>gradhessianv' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R} \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>gradhessianv' f x v</code></strong> returns the original value, the gradient-vector product (directional derivative), and the Hessian-vector product of a vector-to-scalar function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>laplacian : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>laplacian f x</code></strong> returns the <a href="http://en.wikipedia.org/wiki/Laplace_operator#Laplace.E2.80.93Beltrami_operator">Laplacian</a> of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(f(a_1, \dots, a_n): \mathbb{R}^n \to \mathbb{R}\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this gives the sum of second derivatives evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \mathrm{tr}\left(\mathbf{H}_f \right)_\mathbf{x} = \left. \left(\frac{\partial ^2 f}{\partial a_1^2} + \dots + \frac{\partial ^2 f}{\partial a_n^2}\right) \right|_{\mathbf{a} \; = \; \mathbf{x}} \; ,\]</span></p>

<p>which is the trace of the Hessian matrix.</p>

<p>This value can be computed efficiently by the <strong>DiffSharp.AD.Forward2</strong> module, without computing the full Hessian matrix.</p>

<hr />

<h5>laplacian' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R} \times \mathbb{R})}\)</span></h5>

<p><strong><code>laplacian' f x</code></strong> returns the original value and the Laplacian of a vector-to-scalar function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>jacobian : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^{m \times n}}\)</span></h5>

<p><strong><code>jacobian f x</code></strong> returns the <a href="http://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian</a> of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span> with components <span class="math">\(F_1 (a_1, \dots, a_n), \dots, F_m (a_1, \dots, a_n)\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this gives the <span class="math">\(m\)</span>-by-<span class="math">\(n\)</span> Jacobian matrix evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F} \right)_\mathbf{x} = \left. \begin{bmatrix}
                                                            \frac{\partial F_1}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_1}{\partial a_n} \\
                                                            \vdots &amp; \ddots &amp; \vdots  \\
                                                            \frac{\partial F_m}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_n}
                                                           \end{bmatrix} \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<hr />

<h5>jacobian' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^m \times \mathbb{R}^{m \times n})}\)</span></h5>

<p><strong><code>jacobian' f x</code></strong> returns the original value and the Jacobian of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>jacobianv : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^m}\)</span></h5>

<p><strong><code>jacobianv f x v</code></strong> returns the Jacobian-vector product of a vector-to-vector function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span>, and <span class="math">\(\mathbf{x}, \mathbf{v} \in \mathbb{R}^n\)</span>, this gives matrix product of the Jacobian of <span class="math">\(\mathbf{F}\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F} \right)_\mathbf{x} \mathbf{v} \; .\]</span></p>

<p>This value can be computed efficiently by the <strong>DiffSharp.AD.Forward</strong> module in one forward evaluation of the function, in a matrix-free way (without computing the full Jacobian matrix).</p>

<hr />

<h5>jacobianv' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^m \times \mathbb{R}^m)}\)</span></h5>

<p><strong><code>jacobianv' f x v</code></strong> returns the original value and the Jacobian-vector product of a vector-to-vector function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>jacobianT : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}^{n \times m}}\)</span></h5>

<p><strong><code>jacobianT f x</code></strong> returns the transposed Jacobian of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span> with components <span class="math">\(F_1 (a_1, \dots, a_n), \dots, F_m (a_1, \dots, a_n)\)</span>, and <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, this gives the <span class="math">\(n\)</span>-by-<span class="math">\(m\)</span> transposed Jacobian matrix evaluated at <span class="math">\(\mathbf{x}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F}^\textrm{T} \right)_\mathbf{x} = \left. \begin{bmatrix}
                                                            \frac{\partial F_1}{\partial a_1} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_1} \\
                                                            \vdots &amp; \ddots &amp; \vdots  \\
                                                            \frac{\partial F_1}{\partial a_n} &amp; \cdots &amp; \frac{\partial F_m}{\partial a_n}
                                                           \end{bmatrix} \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<hr />

<h5>jacobianT' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^m \times \mathbb{R}^{n \times m})}\)</span></h5>

<p><strong><code>jacobianT' f x</code></strong> returns the original value and the transposed Jacobian of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>jacobianTv : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n \to \mathbb{R}^m} \to \color{blue}{\mathbb{R}^n}\)</span></h5>

<p><strong><code>jacobianTv f x v</code></strong> returns the transposed Jacobian-vector product of a vector-to-vector function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^m\)</span>, <span class="math">\(\mathbf{x} \in \mathbb{R}^n\)</span>, and <span class="math">\(\mathbf{v} \in \mathbb{R}^m\)</span>, this gives the matrix product of the transposed Jacobian of <span class="math">\(\mathbf{F}\)</span> at <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(\mathbf{v}\)</span></p>

<p><span class="math">\[  \left( \mathbf{J}_\mathbf{F}^\textrm{T} \right)_\mathbf{x} \mathbf{v} \; .\]</span></p>

<p>This value can be computed efficiently by the <strong>DiffSharp.AD.Reverse</strong> module in one forward and one reverse evaluation of the function, in a matrix-free way (without computing the full Jacobian matrix).</p>

<hr />

<h5>jacobianTv' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n \to \mathbb{R}^m} \to \color{blue}{(\mathbb{R}^m \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>jacobianTv' f x v</code></strong> returns the original value and the transposed Jacobian-vector product of a vector-to-vector function <code>f</code>, at the point <code>x</code>, along the vector <code>v</code>.</p>

<hr />

<h5>jacobianTv'' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^m \times (\mathbb{R}^m \to \mathbb{R}^n))}\)</span></h5>

<p><strong><code>jacobianTv'' f x</code></strong> returns the original value and a function for evaluating the transposed Jacobian-vector product of a vector-to-vector function <code>f</code>, at point <code>x</code>.</p>

<p>Of the returned pair, the first is the original value of <code>f</code> at the point <code>x</code> (the result of the forward pass of the reverse mode AD) and the second is a function (the reverse evaluator) that can be used to compute the transposed Jacobian-vector product many times along many different vectors (performing a new reverse pass of the reverse mode AD, with the given vector, without repeating the forward pass).</p>

<p>This can be computed efficiently by the <strong>DiffSharp.AD.Reverse</strong> module in a matrix-free way (without computing the full Jacobian matrix).</p>

<hr />

<h5>jacobianvTv : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n \to \mathbb{R}^n \to \mathbb{R}^m} \to \color{blue}{(\mathbb{R}^m \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>jacobianvTv f x v1 v2</code></strong> returns the Jacobian-vector product along the vector <code>v1</code>, and the transposed Jacobian-vector product along the vector<code>v2</code>, of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<p>This can be computed efficiently by the <strong>DiffSharp.AD.ForwardReverse</strong> module in one forward and one reverse evaluation of the function, in a matrix-free way (without computing the full Jacobian matrix).</p>

<hr />

<h5>jacobianvTv' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n \to \mathbb{R}^n \to \mathbb{R}^m} \to \color{blue}{(\mathbb{R}^m \times \mathbb{R}^m \times \mathbb{R}^n)}\)</span></h5>

<p><strong><code>jacobianvTv' f x v1 v2</code></strong> returns the original value, the Jacobian-vector product along the vector <code>v1</code>, and the transposed Jacobian-vector product along the vector <code>v2</code> of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<hr />

<h5>jacobianvTv'' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^m) \to \mathbb{R}^n \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^m \times \mathbb{R}^m \times (\mathbb{R}^m \to \mathbb{R}^n))}\)</span></h5>

<p><strong><code>jacobianvTv'' f x v1</code></strong> returns the original value, the Jacobian-vector product along the vector <code>v1</code>, and a function for evaluating the transposed Jacobian-vector product of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<p>Of the returned 3-tuple, the first is the original value of <code>f</code> at the point <code>x</code>, the second is the Jacobian-vector product of <code>f</code> at the point <code>x</code> along the vector <code>v1</code> (computed using forward mode AD), and the third is a function (the reverse evaluator) that can be used to compute the transposed Jacobian-vector product many times along many different vectors (performing a new reverse pass of reverse mode AD, with the given vector, without repeating the forward pass).</p>

<p>This can be computed efficiently by the <strong>DiffSharp.AD.ForwardReverse</strong> module in a matrix-free way (without computing the full Jacobian matrix).</p>

<h5>curl : <span class="math">\(\color{red}{(\mathbb{R}^3 \to \mathbb{R}^3) \to \mathbb{R}^3} \to \color{blue}{\mathbb{R}^3}\)</span></h5>

<p><strong><code>curl f x</code></strong> returns the <a href="http://en.wikipedia.org/wiki/Curl_(mathematics)">curl</a> of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^3 \to \mathbb{R}^3\)</span> with components <span class="math">\(F_1(a_1, a_2, a_3),\; F_2(a_1, a_2, a_3),\; F_3(a_1, a_2, a_3)\)</span> this gives</p>

<p><span class="math">\[  \left( \textrm{curl} \, \mathbf{F} \right)_{\mathbf{x}} = \left( \nabla \times \mathbf{F} \right)_{\mathbf{x}}= \left. \left[ \frac{\partial F_3}{\partial a_2} - \frac{\partial F_2}{\partial a_3}, \; \frac{\partial F_1}{\partial a_3} - \frac{\partial F_3}{\partial a_1}, \; \frac{\partial F_2}{\partial a_1} - \frac{\partial F_1}{\partial a_2} \right] \right|_{\mathbf{a}\; = \; \mathbf{x}} \; .\]</span></p>

<h5>curl' : <span class="math">\(\color{red}{(\mathbb{R}^3 \to \mathbb{R}^3) \to \mathbb{R}^3} \to \color{blue}{(\mathbb{R}^3 \times \mathbb{R}^3)}\)</span></h5>

<p><strong><code>curl' f x</code></strong> returns the original value and the curl of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<h5>div : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^n) \to \mathbb{R}^n} \to \color{blue}{\mathbb{R}}\)</span></h5>

<p><strong><code>div f x</code></strong> returns the <a href="http://en.wikipedia.org/wiki/Divergence">divergence</a> of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<p>For a function <span class="math">\(\mathbf{F}: \mathbb{R}^n \to \mathbb{R}^n\)</span> with components <span class="math">\(F_1(a_1, \dots, a_n),\; \dots, \; F_n(a_1, \dots, a_n)\)</span> this gives</p>

<p><span class="math">\[  \left( \textrm{div} \, \mathbf{F} \right)_{\mathbf{x}} = \left( \nabla \cdot \mathbf{F} \right)_{\mathbf{x}} = \textrm{tr}\left( \mathbf{J}_{\mathbf{F}} \right)_{\mathbf{x}} = \left. \left( \frac{\partial F_1}{\partial a_1} + \dots + \frac{\partial F_n}{\partial a_n}\right) \right|_{\mathbf{a}\; = \; \mathbf{x}} \; ,\]</span></p>

<p>which is the trace of the Jacobian matrix.</p>

<h5>div' : <span class="math">\(\color{red}{(\mathbb{R}^n \to \mathbb{R}^n) \to \mathbb{R}^n} \to \color{blue}{(\mathbb{R}^n \times \mathbb{R})}\)</span></h5>

<p><strong><code>div' f x</code></strong> returns the original value and the divergence of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<h5>curldiv : <span class="math">\(\color{red}{(\mathbb{R}^3 \to \mathbb{R}^3) \to \mathbb{R}^3} \to \color{blue}{(\mathbb{R}^3 \times \mathbb{R})}\)</span></h5>

<p><strong><code>curldiv f x</code></strong> returns the curl and the divergence of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<h5>curldiv' : <span class="math">\(\color{red}{(\mathbb{R}^3 \to \mathbb{R}^3) \to \mathbb{R}^3} \to \color{blue}{(\mathbb{R}^3 \times \mathbb{R}^3 \times \mathbb{R})}\)</span></h5>

<p><strong><code>curldiv' f x</code></strong> returns the original value, the curl, and the divergence of a vector-to-vector function <code>f</code>, at the point <code>x</code>.</p>

<h2>Implemented Differentiation Techniques</h2>

<p>The main focus of the DiffSharp library is AD, but we also implement symbolic and numerical differentiation.</p>

<p>Currently, the library provides the following implementations in separate modules:</p>

<ul>
<li><strong>DiffSharp.AD.Forward</strong>: Forward mode AD</li>
<li><strong>DiffSharp.AD.Forward2</strong>: Forward mode AD, 2nd order</li>
<li><strong>DiffSharp.AD.ForwardG</strong>: Forward mode AD, keeping vectors of gradient components</li>
<li><strong>DiffSharp.AD.ForwardGH</strong>: Forward mode AD, keeping vectors of gradient components and matrices of Hessian components</li>
<li><strong>DiffSharp.AD.ForwardN</strong>: Forward mode AD, lazy higher-order</li>
<li><strong>DiffSharp.AD.ForwardReverse</strong>: Reverse-on-forward mode AD</li>
<li><strong>DiffSharp.AD.Reverse</strong>: reverse mode AD</li>
<li><strong>DiffSharp.Numerical</strong>: Numerical differentiation</li>
<li><strong>DiffSharp.Symbolic</strong>: Symbolic differentiation</li>
</ul>

<p>For brief explanations of these implementations, please refer to the <a href="gettingstarted-forwardad.html">Forward AD</a>, <a href="gettingstarted-reversead.html">Reverse AD</a>, <a href="gettingstarted-forwardreversead.html">Reverse-on-Forward AD</a>, <a href="gettingstarted-numericaldifferentiation.html">Numerical Differentiation</a>, and <a href="gettingstarted-symbolicdifferentiation.html">Symbolic Differentiation</a> pages.</p>

<h2>Vector and Matrix versus float[] and float[,]</h2>

<p>When a differentiation module such as <strong>DiffSharp.AD.Forward</strong> is opened, the default operations involving vectors or matrices handle these via <strong>float[]</strong> and <strong>float[,]</strong> arrays.</p>

<table class="pre"><tr><td class="lines"><pre class="fssnip">
<span class="l"> 1: </span>
<span class="l"> 2: </span>
<span class="l"> 3: </span>
<span class="l"> 4: </span>
<span class="l"> 5: </span>
<span class="l"> 6: </span>
<span class="l"> 7: </span>
<span class="l"> 8: </span>
<span class="l"> 9: </span>
<span class="l">10: </span>
<span class="l">11: </span>
</pre>
</td>
<td class="snippet"><pre class="fssnip">
<span class="k">open</span> <span onmouseout="hideTip(event, 'fs1', 17)" onmouseover="showTip(event, 'fs1', 17)" class="i">DiffSharp</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs2', 18)" onmouseover="showTip(event, 'fs2', 18)" class="i">AD</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs3', 19)" onmouseover="showTip(event, 'fs3', 19)" class="i">Forward</span>

<span class="c">// Gradient of a vector-to-scalar function</span>
<span class="c">// g1: float[] -&gt; float[]</span>
<span class="c">// i.e. take the function arguments as a float[] and return the gradient as a float[]</span>
<span class="c">// Inner lambda expression: Dual[] -&gt; Dual</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs12', 20)" onmouseover="showTip(event, 'fs12', 20)" class="i">g1</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs13', 21)" onmouseover="showTip(event, 'fs13', 21)" class="i">grad</span> (<span class="k">fun</span> <span onmouseout="hideTip(event, 'fs14', 22)" onmouseover="showTip(event, 'fs14', 22)" class="i">x</span> <span class="k">-&gt;</span> <span onmouseout="hideTip(event, 'fs7', 23)" onmouseover="showTip(event, 'fs7', 23)" class="i">sin</span> (<span onmouseout="hideTip(event, 'fs14', 24)" onmouseover="showTip(event, 'fs14', 24)" class="i">x</span><span class="o">.</span>[<span class="n">0</span>] <span class="o">*</span> <span onmouseout="hideTip(event, 'fs14', 25)" onmouseover="showTip(event, 'fs14', 25)" class="i">x</span><span class="o">.</span>[<span class="n">1</span>]))

<span class="c">// Compute the gradient at (1, 2)</span>
<span class="c">// g1val: float[]</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs15', 26)" onmouseover="showTip(event, 'fs15', 26)" class="i">g1val</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs12', 27)" onmouseover="showTip(event, 'fs12', 27)" class="i">g1</span> [|<span class="n">1.</span>; <span class="n">2.</span>|]</pre>
</td>
</tr>
</table>

<p>In addition to this, every module provides a <strong>Vector</strong> submodule containing versions of the same differentiation operators using the <strong>Vector</strong> and <strong>Matrix</strong> types instead of <strong>float[]</strong> and <strong>float[,]</strong>. This is advantageous in situations where you have to manipulate vectors in the rest of your algorithm. For instance, see the example on <a href="gettingstarted-forwardad.html">Gradient Descent</a>.</p>

<p>Please refer to <a href="reference/index.html">API Reference</a> for a complete list of operations currently supported by the <strong>DiffSharp.Util.LinearAlgebra</strong> module.</p>

<table class="pre"><tr><td class="lines"><pre class="fssnip">
<span class="l"> 1: </span>
<span class="l"> 2: </span>
<span class="l"> 3: </span>
<span class="l"> 4: </span>
<span class="l"> 5: </span>
<span class="l"> 6: </span>
<span class="l"> 7: </span>
<span class="l"> 8: </span>
<span class="l"> 9: </span>
<span class="l">10: </span>
<span class="l">11: </span>
<span class="l">12: </span>
<span class="l">13: </span>
<span class="l">14: </span>
<span class="l">15: </span>
<span class="l">16: </span>
<span class="l">17: </span>
<span class="l">18: </span>
<span class="l">19: </span>
<span class="l">20: </span>
<span class="l">21: </span>
<span class="l">22: </span>
<span class="l">23: </span>
<span class="l">24: </span>
<span class="l">25: </span>
<span class="l">26: </span>
<span class="l">27: </span>
<span class="l">28: </span>
<span class="l">29: </span>
<span class="l">30: </span>
<span class="l">31: </span>
<span class="l">32: </span>
<span class="l">33: </span>
<span class="l">34: </span>
</pre>
</td>
<td class="snippet"><pre class="fssnip">
<span class="k">open</span> <span onmouseout="hideTip(event, 'fs1', 28)" onmouseover="showTip(event, 'fs1', 28)" class="i">DiffSharp</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs2', 29)" onmouseover="showTip(event, 'fs2', 29)" class="i">AD</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs3', 30)" onmouseover="showTip(event, 'fs3', 30)" class="i">Forward</span>
<span class="k">open</span> <span onmouseout="hideTip(event, 'fs1', 31)" onmouseover="showTip(event, 'fs1', 31)" class="i">DiffSharp</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs2', 32)" onmouseover="showTip(event, 'fs2', 32)" class="i">AD</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs3', 33)" onmouseover="showTip(event, 'fs3', 33)" class="i">Forward</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs16', 34)" onmouseover="showTip(event, 'fs16', 34)" class="i">Vector</span>
<span class="k">open</span> <span onmouseout="hideTip(event, 'fs1', 35)" onmouseover="showTip(event, 'fs1', 35)" class="i">DiffSharp</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs17', 36)" onmouseover="showTip(event, 'fs17', 36)" class="i">Util</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs18', 37)" onmouseover="showTip(event, 'fs18', 37)" class="i">LinearAlgebra</span>

<span class="c">// Gradient of a vector-to-scalar function</span>
<span class="c">// g2: Vector&lt;float&gt; -&gt; Vector&lt;float&gt;</span>
<span class="c">// i.e. take the function arguments as a Vector&lt;float&gt; and return the gradient as a Vector&lt;float&gt;</span>
<span class="c">// Inner lambda expression: Vector&lt;Dual&gt; -&gt; Dual</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs19', 38)" onmouseover="showTip(event, 'fs19', 38)" class="i">g2</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs20', 39)" onmouseover="showTip(event, 'fs20', 39)" class="i">grad</span> (<span class="k">fun</span> <span onmouseout="hideTip(event, 'fs21', 40)" onmouseover="showTip(event, 'fs21', 40)" class="i">x</span> <span class="k">-&gt;</span> <span onmouseout="hideTip(event, 'fs7', 41)" onmouseover="showTip(event, 'fs7', 41)" class="i">sin</span> (<span onmouseout="hideTip(event, 'fs21', 42)" onmouseover="showTip(event, 'fs21', 42)" class="i">x</span><span class="o">.</span>[<span class="n">0</span>] <span class="o">*</span> <span onmouseout="hideTip(event, 'fs21', 43)" onmouseover="showTip(event, 'fs21', 43)" class="i">x</span><span class="o">.</span>[<span class="n">1</span>]))

<span class="c">// Compute the gradient at (1, 2)</span>
<span class="c">// g2val: Vector&lt;float&gt;</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs22', 44)" onmouseover="showTip(event, 'fs22', 44)" class="i">g2val</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs19', 45)" onmouseover="showTip(event, 'fs19', 45)" class="i">g2</span> (<span onmouseout="hideTip(event, 'fs23', 46)" onmouseover="showTip(event, 'fs23', 46)" class="i">vector</span> [<span class="n">1.</span>; <span class="n">2.</span>])

<span class="c">// Compute the negative of the gradient vector</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs24', 47)" onmouseover="showTip(event, 'fs24', 47)" class="i">g2valb</span> <span class="o">=</span> <span class="o">-</span><span onmouseout="hideTip(event, 'fs22', 48)" onmouseover="showTip(event, 'fs22', 48)" class="i">g2val</span>

<span class="c">// Scale the gradient vector</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs25', 49)" onmouseover="showTip(event, 'fs25', 49)" class="i">g2valc</span> <span class="o">=</span> <span class="n">0.2</span> <span class="o">*</span> <span onmouseout="hideTip(event, 'fs22', 50)" onmouseover="showTip(event, 'fs22', 50)" class="i">g2val</span>

<span class="c">// Get the norm of the gradient vector</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs26', 51)" onmouseover="showTip(event, 'fs26', 51)" class="i">g2vald</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs27', 52)" onmouseover="showTip(event, 'fs27', 52)" class="i">Vector</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs28', 53)" onmouseover="showTip(event, 'fs28', 53)" class="i">norm</span> <span onmouseout="hideTip(event, 'fs22', 54)" onmouseover="showTip(event, 'fs22', 54)" class="i">g2val</span>

<span class="c">// Jacobian of a vector-to-vector function</span>
<span class="c">// j: Vector&lt;float&gt; -&gt; Matrix&lt;float&gt;</span>
<span class="c">// i.e. take the function arguments as a Vector&lt;float&gt; and return the Jacobian as a Matrix&lt;float&gt;</span>
<span class="c">// Inner lambda expression: Vector&lt;Dual&gt; -&gt; Vector&lt;Dual&gt;</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs29', 55)" onmouseover="showTip(event, 'fs29', 55)" class="i">j</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs30', 56)" onmouseover="showTip(event, 'fs30', 56)" class="i">jacobian</span> (<span class="k">fun</span> <span onmouseout="hideTip(event, 'fs21', 57)" onmouseover="showTip(event, 'fs21', 57)" class="i">x</span> <span class="k">-&gt;</span> <span onmouseout="hideTip(event, 'fs23', 58)" onmouseover="showTip(event, 'fs23', 58)" class="i">vector</span> [<span onmouseout="hideTip(event, 'fs7', 59)" onmouseover="showTip(event, 'fs7', 59)" class="i">sin</span> <span onmouseout="hideTip(event, 'fs21', 60)" onmouseover="showTip(event, 'fs21', 60)" class="i">x</span><span class="o">.</span>[<span class="n">0</span>]; <span onmouseout="hideTip(event, 'fs31', 61)" onmouseover="showTip(event, 'fs31', 61)" class="i">cos</span> <span onmouseout="hideTip(event, 'fs21', 62)" onmouseover="showTip(event, 'fs21', 62)" class="i">x</span><span class="o">.</span>[<span class="n">1</span>]])

<span class="c">// Compute the Jacobian at (1, 2)</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs32', 63)" onmouseover="showTip(event, 'fs32', 63)" class="i">jval</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs29', 64)" onmouseover="showTip(event, 'fs29', 64)" class="i">j</span> (<span onmouseout="hideTip(event, 'fs23', 65)" onmouseover="showTip(event, 'fs23', 65)" class="i">vector</span> [<span class="n">1.</span>; <span class="n">2.</span>])

<span class="c">// Compute the determinant of the Jacobian matrix</span>
<span class="k">let</span> <span onmouseout="hideTip(event, 'fs33', 66)" onmouseover="showTip(event, 'fs33', 66)" class="i">jvalb</span> <span class="o">=</span> <span onmouseout="hideTip(event, 'fs34', 67)" onmouseover="showTip(event, 'fs34', 67)" class="i">Matrix</span><span class="o">.</span><span onmouseout="hideTip(event, 'fs35', 68)" onmouseover="showTip(event, 'fs35', 68)" class="i">det</span> <span onmouseout="hideTip(event, 'fs32', 69)" onmouseover="showTip(event, 'fs32', 69)" class="i">jval</span></pre>
</td>
</tr>
</table>

          <div class="tip" id="fs1">namespace DiffSharp</div>
<div class="tip" id="fs2">namespace DiffSharp.AD</div>
<div class="tip" id="fs3">module Forward<br /><br />from DiffSharp.AD</div>
<div class="tip" id="fs4">val a : float<br /><br />Full name: Api-overview.a</div>
<div class="tip" id="fs5">val diff : f:(Dual -&gt; Dual) -&gt; x:float -&gt; float<br /><br />Full name: DiffSharp.AD.Forward.ForwardOps.diff</div>
<div class="tip" id="fs6">val x : Dual</div>
<div class="tip" id="fs7">val sin : value:&#39;T -&gt; &#39;T (requires member Sin)<br /><br />Full name: Microsoft.FSharp.Core.Operators.sin</div>
<div class="tip" id="fs8">val sqrt : value:&#39;T -&gt; &#39;U (requires member Sqrt)<br /><br />Full name: Microsoft.FSharp.Core.Operators.sqrt</div>
<div class="tip" id="fs9">val b : float<br /><br />Full name: Api-overview.b</div>
<div class="tip" id="fs10">val c : float<br /><br />Full name: Api-overview.c</div>
<div class="tip" id="fs11">val diff&#39; : f:(Dual -&gt; Dual) -&gt; x:float -&gt; float * float<br /><br />Full name: DiffSharp.AD.Forward.ForwardOps.diff&#39;</div>
<div class="tip" id="fs12">val g1 : (float [] -&gt; float [])<br /><br />Full name: Api-overview.g1</div>
<div class="tip" id="fs13">val grad : f:(Dual [] -&gt; Dual) -&gt; x:float [] -&gt; float []<br /><br />Full name: DiffSharp.AD.Forward.ForwardOps.grad</div>
<div class="tip" id="fs14">val x : Dual []</div>
<div class="tip" id="fs15">val g1val : float []<br /><br />Full name: Api-overview.g1val</div>
<div class="tip" id="fs16">module Vector<br /><br />from DiffSharp.AD.Forward</div>
<div class="tip" id="fs17">namespace DiffSharp.Util</div>
<div class="tip" id="fs18">module LinearAlgebra<br /><br />from DiffSharp.Util</div>
<div class="tip" id="fs19">val g2 : (Vector&lt;float&gt; -&gt; Vector&lt;float&gt;)<br /><br />Full name: Api-overview.g2</div>
<div class="tip" id="fs20">val grad : f:(Vector&lt;Dual&gt; -&gt; Dual) -&gt; x:Vector&lt;float&gt; -&gt; Vector&lt;float&gt;<br /><br />Full name: DiffSharp.AD.Forward.Vector.grad</div>
<div class="tip" id="fs21">val x : Vector&lt;Dual&gt;</div>
<div class="tip" id="fs22">val g2val : Vector&lt;float&gt;<br /><br />Full name: Api-overview.g2val</div>
<div class="tip" id="fs23">val vector : v:seq&lt;&#39;a&gt; -&gt; Vector&lt;&#39;a&gt; (requires member get_Zero and member get_One and member ( + ) and member ( - ) and member ( * ) and member ( / ) and member ( ~- ) and member Abs and member Pow and member Sqrt and member op_Explicit and comparison)<br /><br />Full name: DiffSharp.Util.LinearAlgebra.LinearAlgebraOps.vector</div>
<div class="tip" id="fs24">val g2valb : Vector&lt;float&gt;<br /><br />Full name: Api-overview.g2valb</div>
<div class="tip" id="fs25">val g2valc : Vector&lt;float&gt;<br /><br />Full name: Api-overview.g2valc</div>
<div class="tip" id="fs26">val g2vald : float<br /><br />Full name: Api-overview.g2vald</div>
<div class="tip" id="fs27">Multiple items<br />union case Vector.Vector: &#39;T [] -&gt; Vector&lt;&#39;T&gt;<br /><br />--------------------<br />module Vector<br /><br />from DiffSharp.Util.LinearAlgebra<br /><br />--------------------<br />module Vector<br /><br />from DiffSharp.AD.Forward<br /><br />--------------------<br />type Vector&lt;&#39;T (requires member get_Zero and member get_One and member ( + ) and member ( - ) and member ( * ) and member ( / ) and member ( ~- ) and member Abs and member Pow and member Sqrt and member op_Explicit and comparison)&gt; =<br />&#160;&#160;| ZeroVector of &#39;T<br />&#160;&#160;| Vector of &#39;T []<br />&#160;&#160;member Convert : f:(&#39;T -&gt; &#39;a) -&gt; Vector&lt;&#39;a&gt; (requires member get_Zero and member get_One and member ( + ) and member ( - ) and member ( * ) and member ( / ) and member ( ~- ) and member Abs and member Pow and member Sqrt and member op_Explicit and comparison)<br />&#160;&#160;member Copy : unit -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;member GetL1Norm : unit -&gt; &#39;T<br />&#160;&#160;member GetL2Norm : unit -&gt; &#39;T<br />&#160;&#160;member GetL2NormSq : unit -&gt; &#39;T<br />&#160;&#160;member GetLPNorm : p:&#39;T -&gt; &#39;T<br />&#160;&#160;member GetMax : unit -&gt; &#39;T<br />&#160;&#160;member GetMaxBy : f:(&#39;T -&gt; &#39;a1) -&gt; &#39;T (requires comparison)<br />&#160;&#160;member GetMin : unit -&gt; &#39;T<br />&#160;&#160;member GetMinBy : f:(&#39;T -&gt; &#39;a1) -&gt; &#39;T (requires comparison)<br />&#160;&#160;member GetSubVector : s:int * c:int -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;member GetUnitVector : unit -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;member Split : n:int -&gt; seq&lt;Vector&lt;&#39;T&gt;&gt;<br />&#160;&#160;member ToArray : unit -&gt; &#39;T []<br />&#160;&#160;member ToMathematicaString : unit -&gt; string<br />&#160;&#160;member ToMatlabString : unit -&gt; string<br />&#160;&#160;member ToSeq : unit -&gt; seq&lt;&#39;T&gt;<br />&#160;&#160;member FirstItem : &#39;T<br />&#160;&#160;member Item : i:int -&gt; &#39;T with get<br />&#160;&#160;member Length : int<br />&#160;&#160;static member Zero : Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( + ) : a:&#39;T * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( + ) : a:Vector&lt;&#39;T&gt; * b:&#39;T -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( + ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( / ) : a:&#39;T * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( / ) : a:Vector&lt;&#39;T&gt; * b:&#39;T -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( ./ ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( .* ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member op_Explicit : v:Vector&lt;&#39;T&gt; -&gt; float []<br />&#160;&#160;static member ( * ) : a:&#39;T * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( * ) : a:Vector&lt;&#39;T&gt; * b:&#39;T -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( * ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; &#39;T<br />&#160;&#160;static member ( %* ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( - ) : a:&#39;T * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( - ) : a:Vector&lt;&#39;T&gt; * b:&#39;T -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( - ) : a:Vector&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( ~- ) : a:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br /><br />Full name: DiffSharp.Util.LinearAlgebra.Vector&lt;_&gt;</div>
<div class="tip" id="fs28">val norm : v:Vector&lt;&#39;a&gt; -&gt; &#39;a (requires member get_Zero and member get_One and member ( + ) and member ( - ) and member ( * ) and member ( / ) and member ( ~- ) and member Abs and member Pow and member Sqrt and member op_Explicit and comparison)<br /><br />Full name: DiffSharp.Util.LinearAlgebra.Vector.norm</div>
<div class="tip" id="fs29">val j : (Vector&lt;float&gt; -&gt; Matrix&lt;float&gt;)<br /><br />Full name: Api-overview.j</div>
<div class="tip" id="fs30">val jacobian : f:(Vector&lt;Dual&gt; -&gt; Vector&lt;Dual&gt;) -&gt; x:Vector&lt;float&gt; -&gt; Matrix&lt;float&gt;<br /><br />Full name: DiffSharp.AD.Forward.Vector.jacobian</div>
<div class="tip" id="fs31">val cos : value:&#39;T -&gt; &#39;T (requires member Cos)<br /><br />Full name: Microsoft.FSharp.Core.Operators.cos</div>
<div class="tip" id="fs32">val jval : Matrix&lt;float&gt;<br /><br />Full name: Api-overview.jval</div>
<div class="tip" id="fs33">val jvalb : float<br /><br />Full name: Api-overview.jvalb</div>
<div class="tip" id="fs34">Multiple items<br />union case Matrix.Matrix: &#39;T [,] -&gt; Matrix&lt;&#39;T&gt;<br /><br />--------------------<br />module Matrix<br /><br />from DiffSharp.Util.LinearAlgebra<br /><br />--------------------<br />type Matrix&lt;&#39;T (requires member get_Zero and member get_One and member ( + ) and member ( - ) and member ( * ) and member ( / ) and member ( ~- ) and member Abs and member Pow and member Sqrt and member op_Explicit and comparison)&gt; =<br />&#160;&#160;| ZeroMatrix of &#39;T<br />&#160;&#160;| Matrix of &#39;T [,]<br />&#160;&#160;member Copy : unit -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;member GetDeterminant : unit -&gt; &#39;T<br />&#160;&#160;member GetDiagonal : unit -&gt; &#39;T []<br />&#160;&#160;member GetEigenvalues : unit -&gt; &#39;T []<br />&#160;&#160;member GetInverse : unit -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;member GetLUDecomposition : unit -&gt; Matrix&lt;&#39;T&gt; * int [] * &#39;T<br />&#160;&#160;member GetQRDecomposition : unit -&gt; Matrix&lt;&#39;T&gt; * Matrix&lt;&#39;T&gt;<br />&#160;&#160;member GetSlice : rowStart:int option * rowFinish:int option * col:int -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;member GetSlice : row:int * colStart:int option * colFinish:int option -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;member GetSlice : rowStart:int option * rowFinish:int option * colStart:int option * colFinish:int option -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;member GetTrace : unit -&gt; &#39;T<br />&#160;&#160;member GetTranspose : unit -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;member ToArray : unit -&gt; &#39;T [] []<br />&#160;&#160;member ToArray2D : unit -&gt; &#39;T [,]<br />&#160;&#160;member ToMathematicaString : unit -&gt; string<br />&#160;&#160;member ToMatlabString : unit -&gt; string<br />&#160;&#160;member Cols : int<br />&#160;&#160;member Item : i:int * j:int -&gt; &#39;T with get<br />&#160;&#160;member Rows : int<br />&#160;&#160;static member Zero : Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( + ) : a:&#39;T * b:Matrix&lt;&#39;T&gt; -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( + ) : a:Matrix&lt;&#39;T&gt; * b:&#39;T -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( + ) : a:Matrix&lt;&#39;T&gt; * b:Matrix&lt;&#39;T&gt; -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( / ) : a:&#39;T * b:Matrix&lt;&#39;T&gt; -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( / ) : a:Matrix&lt;&#39;T&gt; * b:&#39;T -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( ./ ) : a:Matrix&lt;&#39;T&gt; * b:Matrix&lt;&#39;T&gt; -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( .* ) : a:Matrix&lt;&#39;T&gt; * b:Matrix&lt;&#39;T&gt; -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member op_Explicit : m:Matrix&lt;&#39;T&gt; -&gt; float [,]<br />&#160;&#160;static member ( * ) : a:&#39;T * b:Matrix&lt;&#39;T&gt; -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( * ) : a:Matrix&lt;&#39;T&gt; * b:&#39;T -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( * ) : a:Vector&lt;&#39;T&gt; * b:Matrix&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( * ) : a:Matrix&lt;&#39;T&gt; * b:Vector&lt;&#39;T&gt; -&gt; Vector&lt;&#39;T&gt;<br />&#160;&#160;static member ( * ) : a:Matrix&lt;&#39;T&gt; * b:Matrix&lt;&#39;T&gt; -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( - ) : a:&#39;T * b:Matrix&lt;&#39;T&gt; -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( - ) : a:Matrix&lt;&#39;T&gt; * b:&#39;T -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( - ) : a:Matrix&lt;&#39;T&gt; * b:Matrix&lt;&#39;T&gt; -&gt; Matrix&lt;&#39;T&gt;<br />&#160;&#160;static member ( ~- ) : a:Matrix&lt;&#39;T&gt; -&gt; Matrix&lt;&#39;T&gt;<br /><br />Full name: DiffSharp.Util.LinearAlgebra.Matrix&lt;_&gt;</div>
<div class="tip" id="fs35">val det : m:Matrix&lt;&#39;a&gt; -&gt; &#39;a (requires member get_Zero and member get_One and member ( + ) and member ( - ) and member ( * ) and member ( / ) and member ( ~- ) and member Abs and member Pow and member Sqrt and member op_Explicit and comparison)<br /><br />Full name: DiffSharp.Util.LinearAlgebra.Matrix.det</div>
          
        </div>
        <div class="span3">
          <a href="index.html"><img src="img/diffsharp-logo.png" style="width:140px;height:140px;margin:10px 0px 0px 20px;border-style:none;"/></a>

          <ul class="nav nav-list" id="menu">
            <li class="nav-header">DiffSharp</li>
            <li class="divider"></li>
            <li><a href="index.html">Home Page</a></li>
            <li><a href="https://www.nuget.org/packages/diffsharp">Get DiffSharp via NuGet</a></li>
            <li><a href="http://github.com/gbaydin/DiffSharp">GitHub Page</a></li>
            <li><a href="http://github.com/gbaydin/DiffSharp/releases">Release Notes</a></li>

            <li class="nav-header">Getting Started</li>
            <li class="divider"></li>
            <li><a href="gettingstarted-typeinference.html">Type Inference</a></li>
            <li><a href="api-overview.html">API Overview</a></li>
            <li><a href="gettingstarted-forwardad.html">Forward AD</a></li>
            <li><a href="gettingstarted-reversead.html">Reverse AD</a></li>
            <li><a href="gettingstarted-forwardreversead.html">Reverse-on-Forward AD</a></li>
            <li><a href="gettingstarted-numericaldifferentiation.html">Numerical Differentiation</a></li>
            <li><a href="gettingstarted-symbolicdifferentiation.html">Symbolic Differentiation</a></li>
            
            <li class="nav-header">Benchmarks</li>
            <li class="divider"></li>
            <li><a href="benchmarks.html">Benchmarks</a></li>
            
            <li class="nav-header">Reference</li>
            <li class="divider"></li>
            <li><a href="reference/index.html">API Reference</a></li>

            <li class="nav-header">Examples</li>
            <li class="divider"></li>

            <li class="nav-header">Machine Learning</li>
            <li><a href="examples-gradientdescent.html">Gradient Descent</a></li>
            <li><a href="examples-newtonsmethod.html">Newton's Method</a></li>
            <li><a href="examples-stochasticgradientdescent.html">Stochastic Gradient Descent</a></li>
            <li><a href="examples-kmeansclustering.html">K-Means Clustering</a></li>
            <li><a href="examples-hamiltonianmontecarlo.html">Hamiltonian Monte Carlo</a></li>
            <li><a href="examples-neuralnetworks.html">Neural Networks</a></li>
            <li>Neural Turing Machines (to come)</li>
            <li>Probabilistic Programming<br>(to come)</li>

            <li class="nav-header">Dynamical Systems</li>
            <li>Stability Analysis (to come)</li>

            <li class="nav-header">Control</li>
            <li><a href="examples-inversekinematics.html">Inverse Kinematics</a></li>
            <li>Adaptive Control (to come)</li>

            <li class="nav-header">Physics</li>
            <li><a href="examples-kinematics.html">Kinematics</a></li>
            <li>Leapfrog Integration (to come)</li>
            <li><a href="examples-helmholtzenergyfunction.html">Helmholtz Energy Function</a></li>

            <li class="nav-header">Math</li>
            <li><a href="examples-lhopitalsrule.html">l'Hôpital's Rule</a></li>

            <li class="nav-header">Makers</li>
            <li class="divider"></li>
            <li><a href="http://www.cs.nuim.ie/~gunes/">Atılım Güneş Baydin</a></li>
            <li><a href="http://www.bcl.hamilton.ie/~barak/">Barak A. Pearlmutter</a></li>
            <li><a href="http://www.bcl.hamilton.ie/">Brain and Computation Lab</a></li>
          </ul>
        </div>
      </div>
    </div>
    <!-- Start of StatCounter Code for Default Guide -->
    <script type="text/javascript">
    var sc_project=10059115; 
    var sc_invisible=1; 
    var sc_security="92275ee1"; 
    var scJsHost = (("https:" == document.location.protocol) ?
    "https://secure." : "http://www.");
    document.write("<sc"+"ript type='text/javascript' src='" +
    scJsHost+
    "statcounter.com/counter/counter.js'></"+"script>");
    </script>
    <noscript><div class="statcounter"><a title="web stats"
    href="http://statcounter.com/free-web-stats/"
    target="_blank"><img class="statcounter"
    src="http://c.statcounter.com/10059115/0/92275ee1/1/"
    alt="web stats"></a></div></noscript>
    <!-- End of StatCounter Code for Default Guide -->
  </body>
  </html>